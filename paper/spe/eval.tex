\newcommand{\req}[1]{RQ#1}

\section{Evaluation}\label{sec:eval}

\input{tableRQ1-1}

To show the effectiveness of our approach, we evaluate
\ours with the following two research questions:
\begin{itemize}
  \item \textbf{\req{1}: Feasibility.} Does \ours correctly analyze multilingual
    programs that use various interoperations?

  \item \textbf{\req{2}: Usefulness.} \inred{Is \ours useful for detecting} dataflow-related
    bugs in real-world multilingual programs?
%that the state-of-the-art multilingual program analyzers detect?
\end{itemize}

\subsection{\req{1}: Feasibility}
We evaluate the feasibility of \ours by dataflow analysis on
two benchmark suites for each of Java-C and Python-C analyses.
For the Java-C analysis, we use NativeFlowBench~\cite{nativeflowbench, JN-SAF}
and real-world JNI Android apps downloaded from F-Droid~\cite{fdroid} as the
benchmark suites.
For the Python-C analysis, we use ExtModuleFlowBench that we developed and
real-world C-Python programs downloaded from Github. 

\subsubsection{Feasibility: Java-C}
The first benchmark suite for Java-C program analysis is NativeFlowBench~\cite{nativeflowbench, JN-SAF},
consisting of 23 JNI Android applications (apps) that use various JNI interoperations.
They contain sensitive data leakage from {\it sources} to {\it sinks}
across language boundaries via JNI interoperations.
We compare the analysis results of \ours to those of the state-of-the-art Java-C program
dataflow analyzer, \jnsaf~\cite{JN-SAF}. We use
compiled versions for \jnsaf because it targets compiled JNI programs.

Table~\ref{table:RQ1-1} summarizes the analysis
results of \jnsaf and \ours on NativeFlowBench.
{\bf Benchmark} columns show the benchmark names
and {\bf JN-SAF} and {\bf MultiQL} columns show 
the analysis results of \jnsaf and \ours, respectively.
If an analysis reports all data leakages correctly without any
false positives (FP) or false negatives (FN), it is a success (\cmark).
Otherwise, it is a failure (\xmark).
\ours finds data leakages correctly for 19 benchmarks but reports false
negatives for four benchmarks, while \jnsaf analyzes 21 benchmarks correctly. 
One common failure comes from string concatenation:
{\it native\_complexdata\_stringop} generates a Java field name by
concatenating two string values via a built-in function.
Because \ours does not handle built-in functions, it fails to analyze the benchmark.
{\it icc\_javatonative} and {\it icc\_nativetojava} leak data via the Android
inter-component communication, which is beyond the scope of \ours.
The remaining different failures of \ours and \jnsaf come from
their different array analysis policies.
While both {\it native\_leak\_array} and {\it native\_noleak\_array} store sensitive data in an array,
the former leaks the data, but the latter does not.
Because \jnsaf over-approximates dataflows on arrays,
it analyzes {\it native\_leak\_array} correctly but reports a false positive for {\it native\_noleak\_array}.
By contrast, because CodeQL under-approximates dataflows on arrays,
\ours analyzes {\it native\_noleak\_array} correctly but reports a false negative for
{\it native\_leak\_array}.

The results show that \ours can successfully analyze JNI programs that
use various JNI interoperations, and its correctness is comparable with that of \jnsaf.

%\textbf{Real-world Java-C Programs.} 
The second benchmark suite consists of real-world Java-C Android apps downloaded from
F-Droid, a repository of open-source Android apps~\cite{fdroid}.  We first
downloaded all available apps from F-Droid, and classified downloaded apps into JNI and non-JNI apps.
Then, we selected all 42 apps that can be compiled without any errors as our analysis targets.
For this real-world benchmark, we compare the analysis results of
\ours to those of \lees~\cite{LeeASE20},
the state-of-the-art general-purpose Java-C program analyzer.
We use \lees as a comparison target instead of \jnsaf,
since \jnsaf is not scalable enough to analyze the real-world benchmark.

\input{tableRQ1-2}

\begin{figure}
  \centering
  \vspace{2mm}
  \includegraphics[width=0.65\textwidth]{img/graph}
  \vspace*{-.5em}
  \caption{Analysis time of \ours and \lees}
  \label{fig:graph}
\vspace*{-1em}
\end{figure}

Table~\ref{table:RQ1-2} shows the analysis results of \ours on 25 apps that have
interoperations from C to Java.
The first column shows app names with their indices, the second shows the lines of code,
the third shows database creation time, the
fourth shows query processing time, and the fifth shows the total analysis time.
{\bf C->Java Function Call} and {\bf C->Java Field Access} show the numbers of
C-to-Java function calls and C-to-Java field accesses, respectively. We
collectively call them \emph{JNI uses}.
The sub-columns {\bf \#Precise}, {\bf \#Resolved}, and {\bf Total} represent
the numbers of precisely resolved, resolved, and the total JNI uses,
respectively.
We considered a resolved JNI use as precise, when \ours finds a single target
method or a single field at the JNI use.
\ours resolves 1,076 out of 1,171 (92\%) JNI uses, including 347 out of 404
(86\%) C-to-Java function calls and 729 out of 767 (95\%) C-to-Java field
accesses. 
In addition, 1,008 (86\%) resolved JNI uses are precise.
\ours fails to resolve 95 (8\%) JNI uses because of complex language semantics
such as arrays and function pointers, on which
CodeQL does not track dataflows.

By contrast,
\lees fails to analyze one app because of an error, four apps due to the lack
of memory spaces, and three apps even in eight hours.  
In the remaining 17 apps, \lees resolves 71\% JNI uses
and precisely resolves 46\% of JNI uses,
which shows significantly more imprecise results than \ours.

\ours is scalable in that it can analyze large-scale programs. 
The analysis time was 161.3 seconds on average for each app, including 103.8
seconds for DB creation and 57.5 seconds for query processing.  
DB creation took more time than query processing except for {\it Graph 89}, and
the DB creation time was almost linear to the code size. 
\ours took about 12 minutes at most to analyze {\it Tileless Map} having about
one million lines of code.

Figure~\ref{fig:graph} shows the analysis time of \ours compared to \lees.
The x-axis denotes the indices of apps and the y-axis denotes the analysis time
in seconds. We omit eight apps \lees fails to analyze.
\lees analyzes 12 apps faster than \ours, but the difference is less than two
minutes. 
On the other hand, \ours analyzes five apps faster than \lees, and \lees spends
much time in summary generation for three apps among them. 
In addition, while \lees fails to analyze all the large-scale apps that have
more than about 400,000 lines of code in eight hours, \ours analyzes about
three million lines of code in about 12 minutes.

In summary, the evaluation with the second benchmark suite shows that
\ours can analyze real-world JNI applications more precisely than \lees.
In addition, the scalability of \ours is comparable with \lees in that
it can analyze larger apps much faster than \lees.

\subsubsection{Feasibility: Python-C}
\input{tableRQ1-3}

The first benchmark suite for Python-C program analysis is ExtModuleFlowBench we developed,
consisting of 20 Python-C programs.
We converted each Java-C benchmark in NativeFlowBench into a
corresponding Python-C program, except for eight apps containing JNI-specific interoperation
features. Additionally, we made five Python-C programs that contain interoperations
specific to Python Extension Module.

Table~\ref{table:RQ1-3} summarizes the analysis results of \ours
on 20 Python-C programs in ExtModuleFlowBench.
{\bf Benchmark} columns show the benchmark names and {\bf Dataflow}
columns show the analysis results.
\ours analyzes dataflows correctly for 14 benchmarks, but reports false
negatives for six benchmarks.
\ours fails in {\it complexdata\_stringop} and {\it leak\_array} due to the
same reason for NativeFlowBench.
For the other four benchmarks, \ours fails to track dataflows for the fields of Python objects. 
In the failed benchmarks, the C code changes the values of the fields in Python objects that are passed
to the parameters of C functions.
\ours does not handle such cases because
the use of a virtual argument node prevents the propagation of changed values
back to the field of the original Python object, and it
reports false negatives for them.
The results show that our approach can analyze Python-C programs using various interoperations,
just like it can analyze JNI programs using various JNI interoperations.

\input{tableRQ1-4}

%\textbf{Github}
%\textbf{Real-world Python-C Programs.}
The second benchmark suite consists of \inred{13} real-world Python-C programs
collected from public GitHub repositories. \inred{Six of them} are analysis
targets of Monat et al.~\cite{sas2021}'s work, and \inred{seven of them are
compilable subset of targets of Li et al.~\cite{polycruise}'s work}.

Table~\ref{table:RQ1-4} summarizes the dataflow analysis results on \inred{13}
real-world Python-C programs. 
The first column shows repository names, the second shows the lines of code,
the third shows database creation time,
the fourth shows query processing time, the fifth shows the total analysis time,
and the last two columns denote analysis results.
{\bf \#~Resolved} column denotes the number of resolved API function calls in C code and
{\bf Total} column denotes the total number of Python Extension Module API function
calls in C code. 
Because all the API functions take a Python object as the first argument, we
considered an API function call as resolved, when \ours finds its first argument
correctly. 

\inred{
Out of 13 programs, all programs but one were analyzed within time limit of ?? hours.
The only exception was {\it NumPy}, the largest program in the benchmark suite.
We inspected the evaluation log, and figured out that the reason for the slow analysis for this
program was the inefficeint query optimization of the CodeQL's query system.
}
%{\it python-Levenshtein} contains both Python and C code, but the Python code
%contains only a class that works as a wrapper for C code without any interoperations,
%making no meaningful interaction between Python and C Code.
%{\it distance} and {\it noise} contain too complicated Python code for CodeQL
%to handle.
%
In the remaining three programs, \ours resolves \inred{8,082 out of 11,725 (68.9\%)} total
API function calls. 
\inred{The main for failing to resolve rest of API function calls was the implicit method calls in Python.}
Python code can call some methods implicitly, a class destructor for example, and C
code can register C functions as such implicitly called methods.
Because \ours does not handle C functions implicitly called in Python, it
fails to resolve API function calls in them.

In summary, the evaluation with the second benchmark suite shows that
\ours can analyze real-world Python-C programs with high precision.
It can precisely analyze the Python object values that C variables can have.

\input{tableRQ2}

\subsection{RQ2: Usefulness}
\inred{
We evaluate the usefulness of \ours for detecting interoperation bugs in
real-world Java-C programs and Python-C programs.
For Java-C programs, we detect the actual interoperation bugs from the
real-world JNI android applicatons.
For python-C programs, we show that \ours can extract the dataflow information
that can be utilized by other bug detectors.
}

\subsubsection{Bugs in Java-C programs.}
To show the usefulness of \ours in detecting bugs in Java-C programs,
we search for interoperation bugs from the 42 real-world JNI android applications, downloaded from F-Droid.
We chose to explore the same JNI interoperation bug as those
reported in previous research~\cite{LeeASE20, ILEA}:
\begin{itemize}
  \item {\it NullDeref}: dereferencing the {\tt null} value of Java in C
  \item {\it MissingFun}: calling a missing Java method from C
  \item {\it TypeMismatch}: declaring a C function with a different signature
    from its corresponding Java native method
  \item {\it WrongSig}: calling a Java method using a method ID with a
    wrong signature in C
\end{itemize}
We implemented a checker to detect the bugs on top of \ours using the
query language of CodeQL. 

Table~\ref{table:RQ2} summarizes the bug detection results of 19 out of 42
apps, on which \ours reports possible bugs.  The first
column shows app names, the second and the third columns show the
numbers of true and false positives for {\it NullDeref}, respectively, and
the fourth to the sixth columns show the numbers of true positives for {\it
MissingFun}, {\it TypeMismatch}, and {\it WrongSig}, respectively.
We omit the columns denoting false positivies for the three bugs in the table,
because \ours does not report any false positivies for them.
We confirmed the true and false positives by manual inspection of the source code.
We color the each cell for the number of true positives with non-zero value,
which indicates that the corresponding application had the corresponding kind of bug.

\ours reports 33 genuine bugs and 21 false positives in 19 apps.
It reports five true and 21 false alarms of {\it NullDeref} in 11 apps,
nine true alarms of {\it MissingFun} in three apps,
17 true alarms of {\it TypeMismatch} in eight apps, and
two true alarms of {\it WrongSig} in two apps.
We manually checked that the false positives come from various over-approximation
of the analysis. One of the main causes is conditionally sanitized variables.  Many apps
use a code pattern that assigns a value to a variable only if the variable has
the {\tt null} value. Because our analysis does not support
path-sensitivity that analyzes each execution path separately, it reports that
the variable may still have {\tt null} even after the conditional assignment.
By contrast, \lees fails to detect 12 out of 33 genuine bugs
due to memory or scalability issues during analysis.

\begin{figure}[ht!]
  \centering
%  \vspace{2mm}
  \begin{subfigure}{0.8\textwidth}
    \begin{lstlisting}[style=java,xleftmargin=2.5em]
//EmulatorActivity.java
String tmp = null;
String folder = Util.GetInternalAppStorage(activity);
if (folder != null) {
  tmp = folder + "tmp";
  Util.CreateDirectory(tmp);
}
EmulatorActivity.nativeInitGraph89(..., tmp);
    \end{lstlisting}
    \begin{lstlisting}[style=cpp,xleftmargin=2.5em]
//wrappercommonjni.c
void nativeInitGraph89(..., jstring tmp_dir) {
   (*env)->GetStringUTFChars(env, tmp_dir, 0); ...
}
    \end{lstlisting}
%    \vspace*{-.5em}
    \caption{\normalsize NullDeref}
    \label{fig:bug1}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \begin{lstlisting}[style=java,xleftmargin=2.5em]
//JpegRedaction.java
package org.witness.obscuracam.photo.jpegredaction;

public class JpegRedaction {
  private native void redactRegions(...); ...
}
    \end{lstlisting}
    \begin{lstlisting}[style=cpp,xleftmargin=2.5em]
//JpegRedaction.cpp
void Java_org_witness_securesmartcam_jpegredaction_ JpegRedaction_redactRegions(...) { ... }
    \end{lstlisting}
%    \vspace*{-.5em}
    \caption{\normalsize MissingFun}
    \label{fig:bug2}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \begin{lstlisting}[style=java,xleftmargin=2.5em]
//MuPdfPage.java
private native static List<PageTextBox> search(...);
    \end{lstlisting}
    \begin{lstlisting}[style=cpp,xleftmargin=2.5em]
//mupdfdroidbridge.c
jobjectArray search(...){ ... }
    \end{lstlisting}
%    \vspace*{-.5em}
    \caption{\normalsize TypeMismatch}
    \label{fig:bug3}
  \end{subfigure}
  \begin{subfigure}{0.8\textwidth}
    \begin{lstlisting}[style=java,xleftmargin=2.5em]
//PrBoomActivity.java
void OnMessage(String text);
    \end{lstlisting}
    \begin{lstlisting}[style=cpp,xleftmargin=2.5em]
//jni_doom.h
#define CB_CLASS_MSG_SIG  "(Ljava/lang/String;I)V"

//jni_doom.c
mSendStr = (*env)->GetMethodID(env, jNativesCls,
                       "OnMessage", CB_CLASS_MSG_SIG);
    \end{lstlisting}
%    \vspace*{-.5em}
    \caption{\normalsize WrongSig}
    \label{fig:bug4}
  \end{subfigure}
%  \vspace*{-.5em}
  \caption{Four kinds of JNI interoperation bugs}
  \label{fig:bugs}
\end{figure}

Figure~\ref{fig:bugs} demonstrates four kinds of JNI interoperation bugs \ours
detects from real-world apps. 

Figure~\ref{fig:bug1} is an excerpt from the app, Graph 89, demonstrating {\it
NullDeref} bug.
The Java code may call a C function {\tt nativeInitGraph89} with {\tt
null} as its last argument, because the variable {\tt tmp} has {\tt
null} when the method {\tt GetInternalAppStorage} returns {\tt null}.  The C
function calls a JNI function {\tt GetStringUTFChars} with the value as its
second argument, without checking whether it is {\tt null}. However, because the JNI
specification describes that the second argument of the functions must not be
{\tt null}~\cite{getstringutfchars}, the function may behave unexpectedly.
Note that calling JNI functions with wrong arguments may introduce various
unexpected behaviors, since JVMs do not validate the arguments because of
performance overhead~\cite{hwang2021justgen}.

\renewcommand{\texttt}[1]{%
  \begingroup
  \ttfamily
  \begingroup\lccode`~=`/\lowercase{\endgroup\def~}{/\discretionary{}{}{}}%
  \begingroup\lccode`~=`[\lowercase{\endgroup\def~}{[\discretionary{}{}{}}%
  \begingroup\lccode`~=`.\lowercase{\endgroup\def~}{.\discretionary{}{}{}}%
  \catcode`/=\active\catcode`[=\active\catcode`.=\active
  \scantokens{#1\noexpand}%
  \endgroup
}

Figure~\ref{fig:bug2} is an excerpt from the app, ObscuraCam, demonstrating the
{\it MissingFun} bug.
As described in Section~\ref{sec:merging}, JNI has a C function
naming convention for JVMs to link Java native methods to their corresponding C
functions. While a Java class {\tt JpegRedaction} declaring a native method
{\tt redactRegions} belongs to a package
\texttt{org.witness.obscuracam.photo.jpegredaction}, the corresponding C
function is named with a wrong package name
\texttt{org.witness.securesmartcam.jpegredaction}.  When calling the native
method, JVM fails to link it because of the wrongly named C function.


Figure~\ref{fig:bug3} is an excerpt from the app, Document Viewer,
demonstrating the {\it TypeMismatch} bug.
While the return type of a Java native method {\tt search}
is {\tt List<PageTextBox>}, the return type of the corresponding C function
{\tt search} is {\tt jobjectArray} corresponding to the Java built-in array
container. The return type mismatch has no effect on linking between
Java native methods and C functions. However, since it is an unspecified case in
the JNI specification, interoperations via such native methods may behave
differently on different JVMs~\cite{LeeASE20}. 


Figure~\ref{fig:bug4} is an excerpt from the app, PrBoom, demonstrating the
{\it WrongSig} bug.
The C code tries to get an ID of a Java method {\tt OnMessage} with a
signature \texttt{(LJava/lang/String;I)V}, but the method has a different
signature \texttt{(LJava/lang/String;)V} with one String argument. Because
the signatures do not match, the C code always receives {\tt null}
instead of a method ID, and this example throws a Java exception. In
addition, it may introduce errors in subsequent instructions when using the
return value without null checking or calling JNI functions without handling the
thrown Java exception~\cite{jniexcept}.

The evaluation shows that \ours can detect various JNI interoperation bugs in
real-world JNI apps better than \lees.

\subsubsection{Bugs in Python-C programs.}

\input{tableRQ2-2}

\begin{figure}[ht!]
  \centering
  \vspace{2mm}
  \begin{lstlisting}[style=cpp,xleftmargin=2.5em]
// cvxopt/src/C/cholmod.c
static PyObject* getfactor(PyObject *self, PyObject *args)
{
  if (!PyArg_ParseTuple(args, "O", &F)) return NULL;
  ...
  if (!PyCapsule_CheckExact(F) || !(descr = PyCapsule_GetName(F)))
      err_CO("F");
  if (strncmp(descr, "CHOLMOD FACTOR", 14))
      PY_ERR_TYPE("F is not a CHOLMOD factor");
  ...
}
  \end{lstlisting}
    \vspace*{-.5em}
  \caption{An example of danger-use}
  \label{fig:cvxopt}
\end{figure}

\inred{
To show the usefulness of \ours in detecting interoperation bugs in Python-C
programs, we show that \ours can successfully extract a specfic dataflow
information, which then can be utilized in detecting the actual bugs.  The bugs
we targeted are those used in the work of Li et al.~\cite{polycruise}.  We
observed that the bugs have certain patterns: the value orignated from Python
code is passed to the C code, and then used as either (1) an argument to a
\textit{dangerous} functions, including \ccode{strncpy}, \ccode{strncpm},
\ccode{vsprintf} and \ccode{malloc}, or (2) an index to a array access.  We
will denote these patterns as \textit{inter-uses}.  Note that, an inter-use
itself is not necessarily a bug.  Among the inter-uses, only some of them will
be capable of triggering bugs or security vulnerabilities.  We will denote such
inter-uses as \textit{danger-uses}.  Based on this observation, we implemented
a detector that finds all inter-use, on top of \ours.  Then, one can implement
an actual bug detector by checkingg if a certain inter-use is a danger-use or
not.  Implementing a precise danger-use detector is beyond scope of \ours, so
we only present the result for detecting inter-uses, and check if every known
danger-uses are included in the set of detected inter-uses.

Table~\ref{table:RQ2-2} summarizes the pattern detection out of 12 Python-C
programs that we could analyze.  The first column shows program names, the
second column shows the total number of detected inter-uses, and the third /
fourth columns show number of every detected / known danger-uses, if any. Among
12 programs, total five programs are known to have the danger-uses.

The result shows that out of all nine knwon danger-uses, \ours can corretly
detect eight of them.  Figure~\ref{fig:cvxopt} depicts one danger-use from the
program {\it cvxopt} that \ours could not detect.  The call to the \ccode{strncmp} at
line 8 should be an inter-use, since the first argument \ccode{descr} in
intedned to be originated from the Python code. Also, this call is a
danger-use, because the call to \ccode{strncmp} may incorrectly conclude that
the strings \ccode{descr} and \ccode{"CHOLMOD FACTOR"} are equal, even if they
are not. This is because string terminator (\textquotesingle \textbackslash
0\textquotesingle) is not considered. The correct code should use 15
(\ccode{strlen("CHOLMODE FACTOR")+1}) as the last argument.  \ours could not
detect this inter-use, simply because the function \ccode{getfactor} was
unreachable.  There wasn't any python test files that was calling this
function, so the \ours could not find any python value that \ccode{descr} can
point to. When we manually added a python test file that calls \ccode{getfactor},
\ours correctly reported the call to \ccode{strncmp} in this function as an inter-use.

From the result, we could conclude that \ours is helpful for detecting bugs in
real-world Python-C programs as well, by extrcating the necessary information
required by bug detectors.
}

\subsubsection{Limitation}
\inred{
In this section, we summarize the limitation of \ours that we encountered during the evaluation.

\textbf{Unsoundness} The dataflow steps that models the behavior of inter-language API functions
are implemented by manually referring to the JNI specification and Python's extension module specification.
We implemented a few selected API functions that were enough to practically analyze
all of our benchmark suite, and did not fully covered all of the API functions.
This indeed leads to the ineveitable unsoundness of the dataflow analysis.
One breakthrough will be to use the mechanization specification approach, and
automtically generate the modelling of such API functions automatically.

\textbf{Support to explicit FFI only}
The only way of interoperations between language such that \ours can detect, is via
using explictly FFI calls. However, as (papers that reviewer 2 mentions)~\cite{reviewr2} suggest,
FFI is not the only way of two languages can interact.
Although the benchmarks we used only contains FFI as a method of the interoperation between languages,
identifying other ways of interactions and handling those methods is required for \ours to support more general programs.

\textbf{Dependnecy to underlying declarative analyzer}
Since \ours is implemented on top of CodeQL, the precision and scalibity of \ours heavily depend on the
underlying analyzer, CodeQL. We've encountered some cases where intra-language dataflow analysis is
not precise enough to get the desired result. For example, CodeQL does not support the
analysis of dataflow via variable arguments, so we had to implement on our own.
In addition, we observed that there was one C-Python program, {\it NumPy}, that was not finished
due to the query optimization of CodeQL. 
In order to imporve the performance of \ours, the performance improvement of the
baseline CodeQL should be preceeded.
}
