Dear Editor and Reviewers,

Thank you very much for all your thoughtful and constructive comments. We did our best to address them all.

=================================================================

* Reviewer 1

1-1) The revised version considered and addressed some of the previously raised concerns, such as the soundness issue --- the authors now discussed about this critical matter only in the end of paper when talking about future work. I would suggest this be discussed as part of the technical approach presentation since the fact that the proposed approach is not sound should be clarified up front.
==> We discussed the soundness issue at the beginning of Section 4 and moved the old Section 5.2.3 Future Work to Section 4.6 Discussion.

1-2) Similarly, for another issue, regarding language interfacing mechanisms, only brief discussion is provided in the end, again as part of the future work. I am not sure why the authors could use tools like PolyFax to detect the interfacing and handle it differently --- please at least discuss the difficulties or infeasibility. Note that FFI, although the most obvious and readily-handling type of interfacing, is not the most prevalent, which means the proposed tool would only work with a relatively small portion of the real-world multilingual (e.g., Python-C) programs. At least, discuss the path forward for dealing with this issues, practically and responsibly, rather than simply casting them into future work with brief mentions by passing.
==> We added more discussion about extensibility and moved the old Section 5.2.3 Future Work to Section 4.6 Discussion.

1-3) The other comments are, unfortunately, either only minimally touched upon (without being substantially addressed)---e.g., the benchmark choice, or entirely dismissed (e.g., novelty of the IR, discussion about extensibility to support other analysis or languages with respect to the diversity of the multi-language world).
==> We did our best to address all the comments in the previous response. As we explained in the previous response:
* the benchmark choice: The existing benchmarks, such as PyCBench, do not apply to our tool because our underlying analyzer, CodeQL, cannot support them.
* novelty of the IR: Because IR is not one of the contributions of our work and is irrelevant to the paper's primary goal, we revised Section 2 to remove misunderstandings about IRs.
* discussion about extensibility: We added more discussion about extensibility and moved the old Section 5.2.3 Future Work to Section 4.6 Discussion.

=================================================================

* Reviewer 2

2-1) p1L14. Summary: add "currently" to "do not [currently] support".
==> We revised the abstract as suggested.

2-2) p2L5. "Datalog has become".   This feels wrong.  Datalog was more important than this, with its bottom-up evaluation and its basis in seminal works [as your citations suggest]. Note the paper "codeQuest: Scalable Source Code Queries with Datalog", which shows this (and I think you should cite). Perhaps here (or early) clarify that Datalog was the formal basis which was adapted into various query languages such as CodeQL, QL and SoufflÃ© [personally I still don't know the difference between QL and CodeQL].
==> We emphasized the importance and the role of Datalog as suggested and added a citation for the codeQuest paper. We also clarified the difference between CodeQL (an engine) and QL (a language) in Section 4.1.

2-3) p2 "step 3".  You say "queries are facts which may contain variables".  I agree.  And clearly rules can contain variables.  But (in contrast to Prolog), I suspect all your facts (and derived facts) should *not* contain variables.  If this is true you should state it explicitly.  E.g. p3, line 19 says "replacing variables with their corresponding constants" suggests this.  Please could you review your text to see if this is clear, e.g. that the "facts" box in Fig 1 and Fig 2 are (CodeQL) variable free? -- Clearly they can contain information about program variables, but these are constants from the viewpoint of CodeQL.
==> As suggested, we emphasized that the derived facts do not have free variables.

2-4) P2, line 32.  It would be nice to add a sentence to the effect that "In practical use (and later in the paper) both f() and val would be terms which include line numbers".
==> We added an explanation that `f()` and `val` are "nodes" we defined with their line numbers.

2-5) p4, L28: "as the same" is not English.  Do you mean "The query calledge(X,Y) is valid in multilingual programs as well as monolingual ones, and has the expected meaning"?
==> We revised the sentence as suggested.

2-6) p4L30: interoperation -> interlanguage?
==> We revised the sentence as suggested.

2-7) p4 section 4.1. Please review your explanation of "predicates".  The current explanation seems a little schizophrenic. "predicates" are well known from Prolog and Datalog and indeed logic.   The predicate IsOneOrTwo has a different syntax but is clearly a predicate.  However, you then say (last line): "[Some] predicates may have a return type and a special variable result".  I think you mean "CodeQL allows syntactic definitions of predicates which, like C functions, can have return types; however these are de-sugared into classical datalog-style predicates by adding an additional argument"?  It sort-of says this on the next page, but the explanation is not clear.  Please clarify.  Please could you review the introduction to CodeQL?  'Everyone' knows Prolog/Datalog but knowledge of CodeQL is much less widespread.
==> We revised the explanations of predicates and CodeQL.

2-8) p5 line 9:  "into a *single* trap file"
==> We revised it as suggested.

2-9) p5, sec 4.4: "predicate named viableCallable".  Hmm at the very least please say this is a 'CodeQL predicate' [not a "predicate" as well-known and used in your model in Fig 2].
==> We revised it as suggested.

2-10) p11, line 19.  "precision".  Sorry I did not pick this up on my first review. Precision can refer to both Soundness and Completeness.  For program analysis soundness is vital, but completeness is generally only relative as precise program analyses is uncomputable.  When you say "sacrifice precision", do you mean sacrifice soundness?  Perhaps talk about false positives and false negatives to questions such as "can the indirect call at line 6 invoke function f() defined at line 96"? Lint-style program analysis tools are typically unsound and incomplete.
==> We replaced "precision" with "soundness" and added its implication.

2-11) p19, line 20: "a dangerous functions" -> "a dangerous function"
==> We fixed the typo.

2-12) line 28: could not detect -> did not detect [or cannot detect]
==> We revised it as suggested.

2-13) line 36: "can precisely detect".  Really?  Perhaps remove the word "precisely" or clarify its meaning (see above).  Please review all your uses of "precise".
==> We removed the word "precisely". We kept the word "precise" in explaining Table 2, for example, since we defined its meaning clearly.

2-14) line  -3:  "... various interoperation mechanisms ... exist ... [38]".  Please insert "(such as XXX)" after "mechanisms".  I don't want to read another whole paper to get a feeling for what these might be.
==> We added examples of various interoperation mechanisms.

2-15) Reference 1 has a misplaced "." in the "In:" field. Several references have a strange "<space>." in the author field.  Delete the space. Reference 32 has garbled text and no URL.
==> We revised the references as suggested.

2-16) Finally "Is the abstract concise?" (reviewers are asked this question). The abstract is certainly complete.  On the other hand, it feels rather longer (perhaps 50%) longer than it might be.  I'm happy with the current text but the query-to-reviewers above made me wonder if I should be.  I don't have any immediate suggestions as how to change the abstract, sorry.
==> Because the current abstract contains the main message of the paper and the paper itself is not long, we did not change the abstract.
